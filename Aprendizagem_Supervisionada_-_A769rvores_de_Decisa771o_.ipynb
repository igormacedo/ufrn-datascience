{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tópicos Especiais em Processamento da Informação: Ciência de Dados\n",
    "\n",
    "Prof. Luiz Affonso Guedes Engenharia de Computação - UFRN 2017-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteúdo: \n",
    "- Exercícios de classificação de dados com Classificador Decition Tree.\n",
    "- Exercícios de Random Florest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site do scikit-learn\n",
    "http://scikit-learn.org/stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação dos pacotes padrões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvores de Decisão e Random Florests\n",
    "\n",
    "Material retirado do GitHub do Jake Vanderplas:\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb\n",
    "\n",
    "As árvores de decisão são uma técnica de aprendizado de máquinas poderosa e popular. O conceito básico é muito semelhante às árvores que você pode ter visto comumente usado para auxiliar na tomada de decisões.\n",
    "\n",
    "O algoritmo da árvore de decisão é um algoritmo de aprendizagem supervisionado: primeiro construímos a árvore com dados históricos, e depois usamos isso para prever um resultado.\n",
    "\n",
    "Uma das principais vantagens das árvores de decisão é que elas podem escolher interações não-lineares entre variáveis nos dados, o que a regressão linear não pode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Material retirado do site do scikit-learn:\n",
    "http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "DecisionTreeClassifier é uma classe capaz de realizar classificação multi-class sobre um dataset.\n",
    "\n",
    "Como os outro sclassificadores no scikit-learn, DecisionTreeClassifier tem como entrada 02 arrays: um array X, esparso ou denso, de tamanho [n_samples, n_features] retendo as amostras de treinamento, e um array Y de valores inteiros, de tamanho [n_samples], retendo os labels das classes para as amostras de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo1: Dataset Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de uma árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo predição sobre o modelo treinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier é capaz de fazer classificação binária (quando os labels são [-1, 1]) e classificação multiclasse (quando os labels são [0, …, K-1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o dataset Iris para exemplificar o uso deste classificador como multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de predição\n",
    "clf.predict(iris.data[:1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programa para visualizar a classificaçãomdo dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalação do pacote Graphviz, para apresentação da árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 2: Árvore de Decisão e Random Florest\n",
    "Material retirado do GitHub do Jake Vanderplas:\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo para Apresentação do resultado dos classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helpers_05_08.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "def visualize_tree(estimator, X, y, boundaries=True,\n",
    "                   xlim=None, ylim=None, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis',\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    if xlim is None:\n",
    "        xlim = ax.get_xlim()\n",
    "    if ylim is None:\n",
    "        ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    estimator.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    n_classes = len(np.unique(y))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap='viridis', clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)\n",
    "    \n",
    "    # Plot the decision boundaries\n",
    "    def plot_boundaries(i, xlim, ylim):\n",
    "        if i >= 0:\n",
    "            tree = estimator.tree_\n",
    "        \n",
    "            if tree.feature[i] == 0:\n",
    "                ax.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k', zorder=2)\n",
    "                plot_boundaries(tree.children_left[i],\n",
    "                                [xlim[0], tree.threshold[i]], ylim)\n",
    "                plot_boundaries(tree.children_right[i],\n",
    "                                [tree.threshold[i], xlim[1]], ylim)\n",
    "        \n",
    "            elif tree.feature[i] == 1:\n",
    "                ax.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k', zorder=2)\n",
    "                plot_boundaries(tree.children_left[i], xlim,\n",
    "                                [ylim[0], tree.threshold[i]])\n",
    "                plot_boundaries(tree.children_right[i], xlim,\n",
    "                                [tree.threshold[i], ylim[1]])\n",
    "            \n",
    "    if boundaries:\n",
    "        plot_boundaries(0, xlim, ylim)\n",
    "\n",
    "\n",
    "def plot_tree_interactive(X, y):\n",
    "    def interactive_tree(depth=5):\n",
    "        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "        visualize_tree(clf, X, y)\n",
    "\n",
    "    return interact(interactive_tree, depth=[1, 5])\n",
    "\n",
    "\n",
    "def randomized_tree_interactive(X, y):\n",
    "    N = int(0.75 * X.shape[0])\n",
    "    \n",
    "    xlim = (X[:, 0].min(), X[:, 0].max())\n",
    "    ylim = (X[:, 1].min(), X[:, 1].max())\n",
    "    \n",
    "    def fit_randomized_tree(random_state=0):\n",
    "        clf = DecisionTreeClassifier(max_depth=15)\n",
    "        i = np.arange(len(y))\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        rng.shuffle(i)\n",
    "        visualize_tree(clf, X[i[:N]], y[i[:N]], boundaries=False,\n",
    "                       xlim=xlim, ylim=ylim)\n",
    "    \n",
    "    interact(fit_randomized_tree, random_state=[0, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As figuras a seguir apresentam uma visualização dos primeiros 04 níveis de uma decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_05_08 import visualize_tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 3))\n",
    "fig.subplots_adjust(left=0.02, right=0.98, wspace=0.1)\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "\n",
    "for axi, depth in zip(ax, range(1, 5)):\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    visualize_tree(model, X, y, ax=axi)\n",
    "    axi.set_title('depth = {0}'.format(depth))\n",
    "\n",
    "#fig.savefig('figures/05.08-decision-tree-levels.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este processo de montagem de uma árvore de decisão para nossos dados pode ser feito no Scikit-Learn com o estimador DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para VIsualização do Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado da classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers_05_08 is found in the online appendix\n",
    "import helpers_05_08\n",
    "helpers_05_08.plot_tree_interactive(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que, à medida que a profundidade aumenta, tendemos a obter regiões de classificação muito estranhamente moldadas; por exemplo, em uma profundidade de cinco, há uma região roxa alta e estreita entre as regiões amarela e azul. É claro que isso é menos resultado da distribuição de dados verdadeira e intrínseca, e mais um resultado das propriedades específicas de amostragem ou ruído dos dados. Ou seja, esta árvore de decisão, mesmo com apenas cinco níveis de profundidade, está claramente altamente especializada nos nossos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees e over-fitting\n",
    "\n",
    "Esse excesso de ajuste acaba por ser uma propriedade geral das árvores de decisão: é muito fácil ir muito fundo na árvore e, portanto, ajustar os detalhes dos dados específicos em vez das propriedades gerais das distribuições de que são retirados. Outra maneira de ver isso em excesso é olhar para modelos treinados em diferentes subconjuntos dos dados.\n",
    "\n",
    "A seguir são apresentados resultados com conjuntos de dados distintos, que foram escolhidos randomicamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers_05_08 is found in the online appendix\n",
    "import helpers_05_08\n",
    "helpers_05_08.randomized_tree_interactive(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles of Estimators: Random Forests\n",
    "\n",
    "Essa noção - que os estimadores de superposição múltipla podem ser combinados para reduzir o efeito dessa superposição - é o que está subjacente a um método de conjunto chamado \"bagging\". Bagging faz uso de um conjunto de estimadores paralelos, cada um dos quais supera os dados e mede os resultados para encontrar uma classificação melhor. \n",
    "\n",
    "Um conjunto de árvores de decisão aleatorizada é conhecida como floresta aleatória.\n",
    "\n",
    "Este tipo de classificação pode ser feito manualmente usando o meta-estimador do Ensaio de Bagging do Scikit-Learn, conforme mostrado a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "bag.fit(X, y)\n",
    "visualize_classifier(bag, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, randomizamos os dados ajustando cada estimador com um subconjunto aleatório de 80% dos pontos de treinamento. Na prática, as árvores de decisão são mais efetivamente randomizadas, injetando alguma estocasticidade na forma como as divisões são escolhidas: desta forma, todos os dados contribuem para o ajuste de cada vez, mas os resultados do ajuste ainda possuem aleatoriedade desejada. Por exemplo, ao determinar qual recurso se dividir, a árvore randomizada pode selecionar entre os principais recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Scikit-Learn, um conjunto otimizado de árvores de decisão randomizadas é implementado no estimador RandomForestClassifier, que cuida de toda a randomização automaticamente. Tudo o que você precisa fazer é selecionar uma série de estimadores, e rapidamente (em paralelo, se desejar) se encaixam no conjunto de árvores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site do Scikit-learn: Random Florest:\n",
    "http://scikit-learn.org/stable/modules/ensemble.html#forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "visualize_classifier(model, X, y);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que, com a média de mais de 100 modelos perturbados aleatoriamente, acabamos com um modelo geral que está muito mais próximo de nossa intuição sobre como o espaço de parâmetros deve ser dividido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1: Random Forest para classificação de Dígitos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nos lembrar o que estamos vendo, visualizaremos os primeiros pontos de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos classificar rapidamente os dígitos usando uma Random Florest da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos dar uma olhada no relatório de classificação para este classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, para uma boa medida, trace a matriz de confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2: Dados do Censo Americano\n",
    "\n",
    "Examinaremos a renda individual nos Estados Unidos. Os dados são do censo de 1994 e contêm informações sobre o estado civil, a idade, o tipo de trabalho e mais do indivíduo. A coluna de destino (target), ou o que queremos prever, é predizer se os indivíduos ganham menos ou igual a 50k por ano ou mais de 50k por ano, a partir dos outros dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados podem ser obtidos a partir do website da University of California at Irvine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Set index_col to False to avoid pandas thinking that the first column is row indexes (it's age)\n",
    "#income = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", index_col=False)\n",
    "\n",
    "income = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta as 5 primeira linhas do DataFrame\n",
    "print(income.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta a primeira linha do DataFrame\n",
    "print(income.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inclusão dos nomes das colunas\n",
    "Esse DataFrame não tem nomes de colunas (features e target)\n",
    "- Necessidade de incluir nomes nas colunas.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomes das colunas do DataFrame:\n",
    "Verifique em http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names o significado das colunas.\n",
    "    \n",
    "- age\n",
    "- workclass\n",
    "- fnlwgt\n",
    "- education\n",
    "- education_num\n",
    "- marital_status\n",
    "- occupation\n",
    "- relationship\n",
    "- race\n",
    "- sex\n",
    "- capital_gain\n",
    "- capital_loss\n",
    "- hours_per_week\n",
    "- native_country\n",
    "- high_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusão dos nomes das colunas\n",
    "income.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\",\"capital_loss\", \"hours_per_week\", \"native_country\", \"high_income\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(income.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação do conteúdo da feature \"workclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(income[\"workclass\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a feature é um dado ccategórico (expresso em texto), há a necessidade de convertê-lo em valor numérico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversão de uma coluna de texto categórico para números\n",
    "col = pandas.Categorical.from_array(income[\"workclass\"])\n",
    "income[\"workclass\"] = col.codes\n",
    "print(income[\"workclass\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "O valor 'Private' na coluna 'workclass' foi mapeado para o código numérico 4 (podemos verificar isso comparando os valores na coluna da classe de trabalho que costumava ter o rótulo 'Private' com os valores atuais para ver onde eles se alinham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "private_incomes = income[income[\"workclass\"] == 4]\n",
    "public_incomes = income[income[\"workclass\"] != 4]\n",
    "\n",
    "print(private_incomes.shape)\n",
    "print(public_incomes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executer a linha a baixo e verifique como o Pandas processa as sub-tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income[\"workclass\"] == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instrução:\n",
    "\n",
    "Converte as outras variáveis categóricas d etexto para numérica:\n",
    "- education\n",
    "- marital_status\n",
    "- occupation\n",
    "- relationship\n",
    "- race\n",
    "- sex\n",
    "- native_country\n",
    "- high_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conversão das variáveis categóricas de texto para numérica\n",
    "\n",
    "??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformando Variáveis categórica para o Padrão do SciKitLearn\n",
    "\n",
    "Uma maneira é transformar as variáveis para tipo 'dummy'.\n",
    "\n",
    "- Exemplo: \n",
    "\n",
    "     df = pd.get_dummies(df, columns=['type'])\n",
    "\n",
    "Os códigos categóricos são apenas valores inteiros para os itens exclusivos na categoria dada. Em contraste, get_dummies() retorna uma nova coluna para cada item exclusivo. O valor na coluna indica se a gravação possui ou não esse atributo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemplo de Uso\n",
    "df = pandas.DataFrame({'cat': pandas.Categorical(['a', 'a', 'a', 'b', 'b', 'c'])})\n",
    "df2 = pandas.DataFrame({'cat': [1, 1, 1, 2, 2, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação do conjunto de Teste\n",
    "- Importação do conjunto de teste\n",
    "- Inclusão dos títulos das colunas\n",
    "- Conversão das variáveis textuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "income_test = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", index_col=False)\n",
    "\n",
    "print(income_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inclusão dos títulos das colunas\n",
    "????????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conversão das variáveis categóricas textuais para munéricas\n",
    "\n",
    "????????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o SciKitLearn\n",
    "\n",
    "Podemos usar o pacote scikit-learn para se treinar uma árvore de decisão. A interface é muito semelhante a outros algoritmos de classificação.\n",
    "- Utiliza-se a classe DecisionTreeClassifier para problemas de classificação\n",
    "- Utiliza-se a classe DecisionTreeRegressor para problemas de regressão. \n",
    "- O pacote sklearn.tree inclui ambas as classes.\n",
    "\n",
    "\n",
    "Neste caso, como queremos um resultado binário, usaremos um classificador.\n",
    "\n",
    "O primeiro passo é treinar o classificador nos dados. \n",
    "- Usaremos o método de ajuste em um classificador para fazer isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# A list of columns to train with\n",
    "# We've already converted all columns to numeric\n",
    "features_train = income[[\"age\", \"workclass\",\"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]]\n",
    "target = income[\"high_income\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to make sure the results are consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# We've already loaded the variable \"income,\" which contains all of the income data\n",
    "clf.fit(features_train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando a precisão do modelo\n",
    "\n",
    "Agora que nos treinamos o modelo, podemos fazer previsões. \n",
    "\n",
    "- É preciso dividir os dados em conjuntos de treinamento e testes primeiro. \n",
    "- Se não o fizermos, estaremos fazendo previsões sobre os mesmos dados com os quais treinamos nosso algoritmo.\n",
    "- Isso leva a 'overfit', e fará com que nosso erro pareça mais baixo do que é.\n",
    "\n",
    "Precisamos, então, dividir nossos dados em conjuntos de treinamento e testes primeiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exemplo de predição\n",
    "# utilize o conjunto de tests para fazer as predições\n",
    "\n",
    "from ?????\n",
    "????\n",
    "\n",
    "predictions = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "ypred = predictions\n",
    "ytest = target\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercício: Utilize a matriz de Confusão para analisar a qualidade da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Matriz de confusão do classificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analise a influência de parâmetros sobre desempenho de árvores de decisão\n",
    "- parâmetro \"tamanho do conjunto de treino\"\n",
    "- parâmetro \"número de níveis da árvore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision trees model - parametric performance\n",
    "\n",
    "#clf = DecisionTreeClassifier(random_state=1)\n",
    "#clf = DecisionTreeClassifier(min_samples_split=13, random_state=1)\n",
    "#clf = DecisionTreeClassifier(random_state=1, min_samples_split=13, max_depth=7)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_split=100, max_depth=2)\n",
    "\n",
    "clf.fit(features_train, target)\n",
    "predictions = clf.predict(features_test)\n",
    "\n",
    "ypred = predictions\n",
    "ytest = target\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3: Random Florest\n",
    "- Utilize a técnica de Random Florest para classificar a variável \"high_income\" do exercíco anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resolução do problema\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
